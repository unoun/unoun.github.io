<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Personal Log</title>
    <link>https://unoun.github.io/post/</link>
    <description>Recent content in Posts on Personal Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sun, 05 Jun 2016 17:40:07 +0900</lastBuildDate>
    <atom:link href="https://unoun.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>WiMAX2&#43;とMacのTCP SACKの相性</title>
      <link>https://unoun.github.io/2016/06/compatibility-issue-of-tcp-sack-with-wimax2-and-mac/</link>
      <pubDate>Sun, 05 Jun 2016 17:40:07 +0900</pubDate>
      
      <guid>https://unoun.github.io/2016/06/compatibility-issue-of-tcp-sack-with-wimax2-and-mac/</guid>
      <description>&lt;p&gt;WiMAX2+とSpeed Wi-Fi NEXT W01とMacの組み合わせで、特定のサイトからダウンロードする時に妙な感じに失敗することがあり、
パケットを眺めてみたところ、TCPのSACKに関連があるようだった。&lt;/p&gt;

&lt;p&gt;同じサイトに対してフレッツ回線からは問題なかったところまでは確認できたが、それ以上の問題の切り分けはできなかった。&lt;/p&gt;

&lt;p&gt;原因が確定していないものの、/etc/sysctl.confに次のような設定をして回避している。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;net.inet.tcp.sack=0
net.inet.tcp.delayed_ack=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;どうもSACKを切るとダメなサイトもあるようで、以下の設定と試行錯誤している。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;net.inet.tcp.sack=1
net.inet.tcp.delayed_ack=1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>あなたの代わりに検索します</title>
      <link>https://unoun.github.io/2016/06/they-search-on-your-behalf/</link>
      <pubDate>Fri, 03 Jun 2016 21:42:41 +0900</pubDate>
      
      <guid>https://unoun.github.io/2016/06/they-search-on-your-behalf/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://unoun.github.io/2016/05/the-world-ai-assistant-is-in/&#34; title=&#34;人工知能アシスタントのいる世界&#34;&gt;先の記事&lt;/a&gt;でも触れていることだが、自分でweb検索行為をしなくなった世界を想像してみる。&lt;/p&gt;

&lt;p&gt;web検索そのものは必要だけれど、自ら手を動かしてweb検索することはしない。
また、人工知能アシスタントに何を検索してくださいとは依頼することもない。
伝えるくらいなら、そのまま検索したほうが早いからだ。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;散歩している時に、ふとどこかの庭先で咲いている花の名前を知りたいと思ったとする。
おもむろにポケットからデバイスを取り出したら、既にその花の画像検索結果が表示されている。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;これを実現するためには、視線検出とカメラとオブジェクト認識と検索が連携して常時動作していなければならないので、イマイチ現実味がない。&lt;/p&gt;

&lt;p&gt;アプリを立ち上げ、カメラを向け、開始ボタンを押すとオブジェクト認識が開始されるくらいなら既に存在しているだろう。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;誰かと会話をしている時に、手元にあるデバイスには、常に会話から拾い上げたキーワードとその検索結果が表示されている。
会話内容に合わせて更新されていく画面を見ながら、わからないことで話が止まったりすることもなく話を広げていくことができる。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;音声認識しつつ検索し続けることは可能だろう。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;デバイスで雑誌を読んでいる時に、わからない単語があったとすると、バナー通知で教えてくれる。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;デバイスのカメラで視線検出してスクリーン座標を得ることはできなくはないだろう。
現状の「ペーストして検索」がもう少し使いやすくなるだけでいいのかもしれない。&lt;/p&gt;

&lt;p&gt;さて、これらの共通点は、常に動き続けていることだ。検索結果も使うか使わないかは不明だが、先回りして検索結果を提示しなければならないのだから大変だ。
可能性は感じるが、あまりにも効率が悪い。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>音声アシスタントの可能性</title>
      <link>https://unoun.github.io/2016/06/possibility-of-voice-assistant/</link>
      <pubDate>Wed, 01 Jun 2016 23:32:58 +0900</pubDate>
      
      <guid>https://unoun.github.io/2016/06/possibility-of-voice-assistant/</guid>
      <description>&lt;p&gt;チャットではなく、入力は音声のみを前提とする音声アシスタントの可能性について考えてみた。&lt;/p&gt;

&lt;p&gt;ハンズフリーで音声入力できるデバイスとしては、Amazon Echo、Google Home、iOSのHey Siri機能などが挙げられる。&lt;/p&gt;

&lt;p&gt;仕組みを想像してみると、音声認識のうち、Hey SiriやOK Googleなどの呼び出し部分の音声認識はデバイス単体でやるにしても、それ以上はサーバに音声と付随する様々な情報を送って処理するのが自然だ。&lt;/p&gt;

&lt;p&gt;付随する様々な情報というのは、例えば位置情報だ。今いる場所の近くで美味しいお店を見つけてほしい時に、位置情報がわからなければ探しようがない。予めサーバ側で保持されている利用者のプロフィール情報を使えるなら、より良いレコメンデーションが期待できる。&lt;/p&gt;

&lt;p&gt;この考え方でいくと、音声入力デバイスとサーバ側は一体化されている必要性があり、その結果として一体化されているものが、各社それぞれの音声アシスタント機能ということになる。&lt;/p&gt;

&lt;p&gt;ここで、仮に音声アシスタント機能のプラットフォーム化を目指した場合、音声アシスタント機能を通じて内部的に呼び出されるサービス機能が何らかの形で開放されることが考えられる。&lt;/p&gt;

&lt;p&gt;その場合は、1つのアシスタントに様々なサービス機能を共存させる必要が生まれるため、各サービス機能を呼び出すためのキーワードと、キーワードに対するパラメータという形をとることになる。&lt;/p&gt;

&lt;p&gt;結果として、1対1で音声アシスタントと会話することになり、かつ音声アシスタントとして多くのサービス機能を提供することになる。
これは良くも悪くもボット的な動作になる。誤認識してしまった時のストレスなどを考えると、利用者として内部動作が想像しやすいボット的な動作のほうが現実的とさえ思う。&lt;/p&gt;

&lt;p&gt;別の観点として、据え置き型のデバイスにおいて、音声アシスタント側から会話を開始するケースを想像してみる。&lt;/p&gt;

&lt;p&gt;チャットとは違い、音声はその場限りなのだから、その時その場で必要とされる事柄について話しかけてくれる必要があるだろう。
そうでなければ、うるさい存在になってしまう。&lt;/p&gt;

&lt;p&gt;その時その場で必要とされる事柄を判断するには、先にも述べているように音声入力に付随する様々な情報が必要になる。
そのため、音声アシスタント側から会話を開始するケースでは、マイクとスピーカーだけあればいいということにはならない。デバイス本体に持たずとも、ホームネットワークとしての屋内外のカメラや人感センサーとの接続は十分考えられる。&lt;/p&gt;

&lt;p&gt;カメラの情報から顔認識と表情推定まで本体内で行えるなら、たとえば表情推定に応じた音楽を流せるかもしれない。&lt;/p&gt;

&lt;p&gt;IFTTTのようなものでサーバ側に情報を送れて、それを判断材料にできるなら、イベント契機で話しかけてくれる仕組みは可能だ。
処理フローの途中に、音声通知なり、実行確認を求めてキャンセル可能にするような使い方もできる。&lt;/p&gt;

&lt;p&gt;そうやって拡張を続けていけば、できることは増えていく。ただし労力に見合った利便性を得られるかというと、そこは疑問だ。
ということは、音声アシスタント側から会話を開始するケースは今のところ考えないほうがよさそうだ。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mac版GnuCashの帳票の日本語表示をなおす方法</title>
      <link>https://unoun.github.io/2016/05/japanese-font-of-report-on-gnucash-for-mac/</link>
      <pubDate>Tue, 31 May 2016 07:19:09 +0900</pubDate>
      
      <guid>https://unoun.github.io/2016/05/japanese-font-of-report-on-gnucash-for-mac/</guid>
      <description>&lt;p&gt;デフォルト状態では帳票の日本語が一切表示されない。
しかも、メニューから辿れる範囲のフォントを指定してみても効果がない。&lt;/p&gt;

&lt;p&gt;どうやら帳票部分はデフォルトのフォントが使われ、しかもUnicodeフォントでなければ日本語が表示されないようだ。&lt;/p&gt;

&lt;p&gt;そこで、以下のように ~/.gtkrc-2.0.gnucash を書くことによって、デフォルトをUnicodeフォントにして、他は通常の日本語フォントを設定している。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#
style &amp;quot;GtkDefault&amp;quot; {
  font_name = &amp;quot;Arial Unicode MS&amp;quot;
}
widget_class &amp;quot;*&amp;quot; style &amp;quot;GtkDefault&amp;quot;

#
style &amp;quot;gnc-register&amp;quot; {
  font_name = &amp;quot;Hiragino Sans W2&amp;quot;
}
widget &amp;quot;*.GnucashSheet&amp;quot; style &amp;quot;gnc-register&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>人工知能アシスタントのいる世界</title>
      <link>https://unoun.github.io/2016/05/the-world-ai-assistant-is-in/</link>
      <pubDate>Thu, 26 May 2016 07:28:54 +0900</pubDate>
      
      <guid>https://unoun.github.io/2016/05/the-world-ai-assistant-is-in/</guid>
      <description>&lt;p&gt;そういう世界になりそうなので、なった時のことを想像してみる。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;チャットアプリでは、人工無脳的に反応してくれるボットと、パーソナライズされた専用の人工知能アシスタントが待機していて、私たちの入力を待っている。&lt;/p&gt;

&lt;p&gt;ボットは、今でも実現できているように、スケジュールの確認や追加をしたり、ピザを頼んだりできる。&lt;/p&gt;

&lt;p&gt;今までと違うのは、人工知能アシスタントが、私たちに代わってこれらのボットやweb上のAPIを使い、これなんだっけと検索してみようと思った時は空気を読んで検索結果を教えてくれたり、口論になりそうになったら場を和ませてくれるようなことを言ってくれたりすることだ。必要とあれば無駄話だってしてくれる。&lt;/p&gt;

&lt;p&gt;人工知能アシスタントには設定項目という概念はない。最初にどの人工知能アシスタントと契約するかは重要だが、こうしてほしいああしてほしいなどは、会話を通じて伝え、学習してもらう。&lt;/p&gt;

&lt;p&gt;人工知能を通じてできることは、学習過程に応じて変わっていくものなので、必要に応じてボットやAPIのパーミッションを変えていく。&lt;/p&gt;

&lt;p&gt;学習段階の人口知能が失敗してもいいように、大抵のボットやAPIには練習モードが備わっている。&lt;/p&gt;

&lt;p&gt;ボットも人工知能アシスタントも稼働させるのはタダではないため、料金を支払って借りるのが一般的だ。ただしボットのほうは広告付きを使うことが多い。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;さて、このような世界になったとして、人工知能アシスタントに料金を支払って借りるだけの価値が生まれるとしたら、それはどのような時だろうか。OSに組み込まれていたりチャットアプリ上に存在するだけでは難しい。&lt;/p&gt;

&lt;p&gt;例えばSiriに料金を支払ってまで世話になろうとは思わないだろう。&lt;/p&gt;

&lt;p&gt;例えばEchoのようなデバイスに人工知能アシスタントが存在していたとして、商品購入前に、似たような商品含めて比較したり評判を調べてくれたとしても、それだけでは不十分だ。&lt;/p&gt;

&lt;p&gt;こちらが尋ねるまでもなく適切なタイミングで天気予報をチェックして傘が必要なことを知らせてくれるとしても、それだけはやはり不十分だ。&lt;/p&gt;

&lt;p&gt;人間がその時思いもしなかったことを前もってアシストしてくれるようになった時に、その価値が生まれると思う。&lt;/p&gt;

&lt;p&gt;従って、逆説的ではあるが、人工知能アシスタントはその価値が生まれるまで、無料のまま、あまり人工知能的ではないかもしれないささやかな機能を1つ1つ積み重ねていくことになるだろう。&lt;/p&gt;

&lt;p&gt;思いつく限りのささやかな機能の積み重ねの果てに、人工知能アシスタントのいる世界がある。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GitHub Pagesはじめました</title>
      <link>https://unoun.github.io/2016/05/i-started-with-github-pages/</link>
      <pubDate>Wed, 25 May 2016 07:17:38 +0900</pubDate>
      
      <guid>https://unoun.github.io/2016/05/i-started-with-github-pages/</guid>
      <description>&lt;p&gt;気になる情報や、空想などを書いていこうと思います。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Personal Log</title>
    <link>https://unoun.github.io/post/</link>
    <description>Recent content in Posts on Personal Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Wed, 01 Jun 2016 23:32:58 +0900</lastBuildDate>
    <atom:link href="https://unoun.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>音声アシスタントの可能性</title>
      <link>https://unoun.github.io/2016/06/possibility-of-voice-assistant/</link>
      <pubDate>Wed, 01 Jun 2016 23:32:58 +0900</pubDate>
      
      <guid>https://unoun.github.io/2016/06/possibility-of-voice-assistant/</guid>
      <description>&lt;p&gt;チャットではなく、入力は音声のみを前提とする音声アシスタントの可能性について考えてみた。&lt;/p&gt;

&lt;p&gt;ハンズフリーで音声入力できるデバイスとしては、Amazon Echo、Google Home、iOSのHey Siri機能などが挙げられる。&lt;/p&gt;

&lt;p&gt;仕組みを想像してみると、音声認識のうち、Hey SiriやOK Googleなどの呼び出し部分の音声認識はデバイス単体でやるにしても、それ以上はサーバに音声と付随する様々な情報を送って処理するのが自然だ。&lt;/p&gt;

&lt;p&gt;付随する様々な情報というのは、例えば位置情報だ。今いる場所の近くで美味しいお店を見つけてほしい時に、位置情報がわからなければ探しようがない。予めサーバ側で保持されている利用者のプロフィール情報を使えるなら、より良いレコメンデーションが期待できる。&lt;/p&gt;

&lt;p&gt;この考え方でいくと、音声入力デバイスとサーバ側は一体化されている必要性があり、その結果として一体化されているものが、各社それぞれの音声アシスタント機能ということになる。&lt;/p&gt;

&lt;p&gt;ここで、仮に音声アシスタント機能のプラットフォーム化を目指した場合、音声アシスタント機能を通じて内部的に呼び出されるサービス機能が何らかの形で開放されることが考えられる。&lt;/p&gt;

&lt;p&gt;その場合は、1つのアシスタントに様々なサービス機能を共存させる必要が生まれるため、各サービス機能を呼び出すためのキーワードと、キーワードに対するパラメータという形をとることになる。&lt;/p&gt;

&lt;p&gt;結果として、1対1で音声アシスタントと会話することになり、かつ音声アシスタントとして多くのサービス機能を提供することになる。
これは良くも悪くもボット的な動作になる。誤認識してしまった時のストレスなどを考えると、利用者として内部動作が想像しやすいボット的な動作のほうが現実的とさえ思う。&lt;/p&gt;

&lt;p&gt;別の観点として、据え置き型のデバイスにおいて、音声アシスタント側から会話を開始するケースを想像してみる。&lt;/p&gt;

&lt;p&gt;チャットとは違い、音声はその場限りなのだから、その時その場で必要とされる事柄について話しかけてくれる必要があるだろう。
そうでなければ、うるさい存在になってしまう。&lt;/p&gt;

&lt;p&gt;その時その場で必要とされる事柄を判断するには、先にも述べているように音声入力に付随する様々な情報が必要になる。
そのため、音声アシスタント側から会話を開始するケースでは、マイクとスピーカーだけあればいいということにはならない。デバイス本体に持たずとも、ホームネットワークとしての屋内外のカメラや人感センサーとの接続は十分考えられる。&lt;/p&gt;

&lt;p&gt;カメラの情報から顔認識と表情推定まで本体内で行えるなら、たとえば表情推定に応じた音楽を流せるかもしれない。&lt;/p&gt;

&lt;p&gt;IFTTTのようなものでサーバ側に情報を送れて、それを判断材料にできるなら、イベント契機で話しかけてくれる仕組みは可能だ。
処理フローの途中に、音声通知なり、実行確認を求めてキャンセル可能にするような使い方もできる。&lt;/p&gt;

&lt;p&gt;そうやって拡張を続けていけば、できることは増えていく。ただし労力に見合った利便性を得られるかというと、そこは疑問だ。
ということは、音声アシスタント側から会話を開始するケースは今のところ考えないほうがよさそうだ。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mac版GnuCashの帳票の日本語表示をなおす方法</title>
      <link>https://unoun.github.io/2016/05/japanese-font-of-report-on-gnucash-for-mac/</link>
      <pubDate>Tue, 31 May 2016 07:19:09 +0900</pubDate>
      
      <guid>https://unoun.github.io/2016/05/japanese-font-of-report-on-gnucash-for-mac/</guid>
      <description>&lt;p&gt;デフォルト状態では帳票の日本語が一切表示されない。
しかも、メニューから辿れる範囲のフォントを指定してみても効果がない。&lt;/p&gt;

&lt;p&gt;どうやら帳票部分はデフォルトのフォントが使われ、しかもUnicodeフォントでなければ日本語が表示されないようだ。&lt;/p&gt;

&lt;p&gt;そこで、以下のように ~/.gtkrc-2.0.gnucash を書くことによって、デフォルトをUnicodeフォントにして、他は通常の日本語フォントを設定している。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#
style &amp;quot;GtkDefault&amp;quot; {
  font_name = &amp;quot;Arial Unicode MS&amp;quot;
}
widget_class &amp;quot;*&amp;quot; style &amp;quot;GtkDefault&amp;quot;

#
style &amp;quot;gnc-register&amp;quot; {
  font_name = &amp;quot;Hiragino Sans W2&amp;quot;
}
widget &amp;quot;*.GnucashSheet&amp;quot; style &amp;quot;gnc-register&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>人工知能アシスタントのいる世界</title>
      <link>https://unoun.github.io/2016/05/the-world-ai-assistant-is-in/</link>
      <pubDate>Thu, 26 May 2016 07:28:54 +0900</pubDate>
      
      <guid>https://unoun.github.io/2016/05/the-world-ai-assistant-is-in/</guid>
      <description>&lt;p&gt;そういう世界になりそうなので、なった時のことを想像してみる。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;チャットアプリでは、人工無脳的に反応してくれるボットと、パーソナライズされた専用の人工知能アシスタントが待機していて、私たちの入力を待っている。&lt;/p&gt;

&lt;p&gt;ボットは、今でも実現できているように、スケジュールの確認や追加をしたり、ピザを頼んだりできる。&lt;/p&gt;

&lt;p&gt;今までと違うのは、人工知能アシスタントが、私たちに代わってこれらのボットやweb上のAPIを使い、これなんだっけと検索してみようと思った時は空気を読んで検索結果を教えてくれたり、口論になりそうになったら場を和ませてくれるようなことを言ってくれたりすることだ。必要とあれば無駄話だってしてくれる。&lt;/p&gt;

&lt;p&gt;人工知能アシスタントには設定項目という概念はない。最初にどの人工知能アシスタントと契約するかは重要だが、こうしてほしいああしてほしいなどは、会話を通じて伝え、学習してもらう。&lt;/p&gt;

&lt;p&gt;人工知能を通じてできることは、学習過程に応じて変わっていくものなので、必要に応じてボットやAPIのパーミッションを変えていく。&lt;/p&gt;

&lt;p&gt;学習段階の人口知能が失敗してもいいように、大抵のボットやAPIには練習モードが備わっている。&lt;/p&gt;

&lt;p&gt;ボットも人工知能アシスタントも稼働させるのはタダではないため、料金を支払って借りるのが一般的だ。ただしボットのほうは広告付きを使うことが多い。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;さて、このような世界になったとして、人工知能アシスタントに料金を支払って借りるだけの価値が生まれるとしたら、それはどのような時だろうか。OSに組み込まれていたりチャットアプリ上に存在するだけでは難しい。&lt;/p&gt;

&lt;p&gt;例えばSiriに料金を支払ってまで世話になろうとは思わないだろう。&lt;/p&gt;

&lt;p&gt;例えばEchoのようなデバイスに人工知能アシスタントが存在していたとして、商品購入前に、似たような商品含めて比較したり評判を調べてくれたとしても、それだけでは不十分だ。&lt;/p&gt;

&lt;p&gt;こちらが尋ねるまでもなく適切なタイミングで天気予報をチェックして傘が必要なことを知らせてくれるとしても、それだけはやはり不十分だ。&lt;/p&gt;

&lt;p&gt;人間がその時思いもしなかったことを前もってアシストしてくれるようになった時に、その価値が生まれると思う。&lt;/p&gt;

&lt;p&gt;従って、逆説的ではあるが、人工知能アシスタントはその価値が生まれるまで、無料のまま、あまり人工知能的ではないかもしれないささやかな機能を1つ1つ積み重ねていくことになるだろう。&lt;/p&gt;

&lt;p&gt;思いつく限りのささやかな機能の積み重ねの果てに、人工知能アシスタントのいる世界がある。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GitHub Pagesはじめました</title>
      <link>https://unoun.github.io/2016/05/i-started-with-github-pages/</link>
      <pubDate>Wed, 25 May 2016 07:17:38 +0900</pubDate>
      
      <guid>https://unoun.github.io/2016/05/i-started-with-github-pages/</guid>
      <description>&lt;p&gt;気になる情報や、空想などを書いていこうと思います。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>